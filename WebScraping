1. Scrap all the jobs in https://realpython.github.io/fake-jobs/
import requests
from bs4 import BeautifulSoup
url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
jobs = soup.find_all("div", class_="card-content")
for job in jobs:
 print(job.prettify())

2. Modify Q1 to scrap only python related job postings
import requests
from bs4 import BeautifulSoup
url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
jobs = soup.find_all("div", class_="card-content")
for job in jobs:
 title = job.find("h2", class_="title").text.strip()
 if "python" in title.lower():
 print(job.prettify())

3. Modify Q2 to get information about Job Title, Company and location
import requests
from bs4 import BeautifulSoup
url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
jobs = soup.find_all("div", class_="card-content")
for job in jobs:
 title = job.find("h2", class_="title").text.strip()
 if "python" in title.lower():
 company = job.find("h3", class_="company").text.strip()
 location = job.find("p", class_="location").text.strip()
 print(f"Job Title: {title}\nCompany: {company}\nLocation: {location}\n")

4. Modify Q2 to get Link (url) of all python related jobs to apply
import requests
from bs4 import BeautifulSoup
url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
jobs = soup.find_all("div", class_="card-content")
for job in jobs:
 title = job.find("h2", class_="title").text.strip()
 if "python" in title.lower():
 apply_link = job.find("a", string="Apply")["href"]
 print(f"{title} - Apply here: {apply_link}")

5. Scrap all information related to Indian universities in
https://en.wikipedia.org/wiki/List_of_universities_in_India. Store the scraped data in CSV
file.
import pandas as pd
url = "https://en.wikipedia.org/wiki/List_of_universities_in_India"
tables = pd.read_html(url)
df = pd.concat(tables, ignore_index=True)
df.to_csv("indian_universities.csv", index=False)
print("University data has been saved to indian_universities.csv")
